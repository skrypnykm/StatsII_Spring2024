\documentclass[12pt,letterpaper]{article}
\usepackage{graphicx,textcomp}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{fullpage}
\usepackage{color}
\usepackage[reqno]{amsmath}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{amssymb,enumerate}
\usepackage[all]{xy}
\usepackage{endnotes}
\usepackage{lscape}
\newtheorem{com}{Comment}
\usepackage{float}
\usepackage{hyperref}
\newtheorem{lem} {Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{cor}{Corollary}
\newtheorem{obs}{Observation}
\usepackage[compact]{titlesec}
\usepackage{dcolumn}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{multirow}
\usepackage{xcolor}
\newcolumntype{.}{D{.}{.}{-1}}
\newcolumntype{d}[1]{D{.}{.}{#1}}
\definecolor{light-gray}{gray}{0.65}
\usepackage{url}
\usepackage{listings}
\usepackage{color}
\usepackage{enumitem}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}
\lstset{style=mystyle}
\newcommand{\Sref}[1]{Section~\ref{#1}}
\newtheorem{hyp}{Hypothesis}

\title{Problem Set 1}
\date{Due: February 11, 2024}
\author{Applied Stats II \\ \vspace{\baselineskip}
	\textbf{Maiia Skrypnyk 23371609}}

\begin{document}
	\maketitle
	\section*{Instructions}
	\begin{itemize}
	\item Please show your work! You may lose points by simply writing in the answer. If the problem requires you to execute commands in \texttt{R}, please include the code you used to get your answers. Please also include the \texttt{.R} file that contains your code. If you are not sure if work needs to be shown for a particular problem, please ask.
\item Your homework should be submitted electronically on GitHub in \texttt{.pdf} form.
\item This problem set is due before 23:59 on Sunday February 11, 2024. No late assignments will be accepted.
	\end{itemize}

	\vspace{.25cm}
\section*{Question 1} 
\vspace{.25cm}
\noindent The Kolmogorov-Smirnov test uses cumulative distribution statistics test the similarity of the empirical distribution of some observed data and a specified PDF, and serves as a goodness of fit test. The test statistic is created by:

$$D = \max_{i=1:n} \Big\{ \frac{i}{n}  - F_{(i)}, F_{(i)} - \frac{i-1}{n} \Big\}$$

\noindent where $F$ is the theoretical cumulative distribution of the distribution being tested and $F_{(i)}$ is the $i$th ordered value. Intuitively, the statistic takes the largest absolute difference between the two distribution functions across all $x$ values. Large values indicate dissimilarity and the rejection of the hypothesis that the empirical distribution matches the queried theoretical distribution. The p-value is calculated from the Kolmogorov-
Smirnoff CDF:

$$p(D \leq d)= \frac{\sqrt {2\pi}}{d} \sum _{k=1}^{\infty }e^{-(2k-1)^{2}\pi ^{2}/(8d^{2})}$$


\noindent which generally requires approximation methods (see \href{https://core.ac.uk/download/pdf/25787785.pdf}{Marsaglia, Tsang, and Wang 2003}). This so-called non-parametric test (this label comes from the fact that the distribution of the test statistic does not depend on the distribution of the data being tested) performs poorly in small samples, but works well in a simulation environment. Write an \texttt{R} function that implements this test where the reference distribution is normal. Using \texttt{R} generate 1,000 Cauchy random variables (\texttt{rcauchy(1000, location = 0, scale = 1)}) and perform the test (remember, use the same seed, something like \texttt{set.seed(123)}, whenever you're generating your own data).\\
	
	
\noindent As a hint, you can create the empirical distribution and theoretical CDF using this code:

\begin{lstlisting}[language=R]
	# create empirical distribution of observed dat
	ECDF <- ecdf(data)
	empiricalCDF <- ECDF(data)
	# generate test statistic
	D <- max(abs(empiricalCDF - pnorm(data))) \end{lstlisting}
	
	Given the above-mentioned formulas for the test-statistic and the p-value, I followed the next algorithm to create a function for the Kosmogorov-Smirnov test in R:
	
\begin{enumerate}[noitemsep]
	\item Create empirical distribution of observed data;
	\item Generate test statistic;
	\item Indicate number of observations $\textit{n}$;
	\item Indicate a sequence of indices $\textit{k}$;
	\item Calculating the p-value\footnote{Calculating the sum of exponential terms first for further simplicity};
	\item The function should return values for the test-statistic $D$ and the p-value.
\end{enumerate}
	
	\lstinputlisting[language=R, firstline=40, lastline=49]{PS1MS.R} 
	
Setting the seed and generating 1,000 Cauchy random variables:
	
		\lstinputlisting[language=R, firstline=51, lastline=52]{PS1MS.R} 
		
Performing the Kosmogorov-Smirnov test using the manually coded function:
		\lstinputlisting[language=R, firstline=54, lastline=54]{PS1MS.R} 
		
	\begin{verbatim}
		$D
		[1] 0.1347281
		
		$p_value
		[1] 5.652523e-29
	\end{verbatim}
	
	Comparing the results to the built-in R function: 
	
		\lstinputlisting[language=R, firstline=56, lastline=56]{PS1MS.R} 
		
\begin{verbatim}
	Asymptotic one-sample Kolmogorov-Smirnov test
	
	data: variables
	D = 0.13573, p-value = 2.22e-16
	alternative hypothesis: two-sided
\end{verbatim}

\textbf{Conclusions:}
\begin{enumerate}[noitemsep]
\item The values of the test-statistic \textit{D } are quite close to each other, so we might suggest the difference occured due to rounding during the computing process.
\item The calculated p-values are extra small and close to zero ("essentially zero"). At the conventional threshold of statistical significance 0.05, or even 0.01, such small p-values (though different numbers) indicate that we have evidence to reject the null hypothesis of these two distributions' similarity -- which makes total sense.
\end{enumerate}


\section*{Question 2}
\noindent Estimate an OLS regression in \texttt{R} that uses the Newton-Raphson algorithm (specifically \texttt{BFGS}, which is a quasi-Newton method), and show that you get the equivalent results to using \texttt{lm}. Use the code below to create your data.
\lstinputlisting[language=R, firstline=62, lastline=64]{PS1MS.R} 
\vspace{.5cm}


As we know, OLS (Ordinary Least Squares) method minimizes the sum of squared errors (SSE). Sum of squared errors = the sum of squared differences between actual and predicted values of \textit{Y},so it is the sum of the squared residuals.

General formula for the linear model is given by $\hat{y} = \beta_0 + \beta_1 x$, and for our data it is \\ \textit{y} = 0+2.75\textit{x}.

So, the logic behind my manually coded OLS regression function (pre-optimisation) would be:
\begin{enumerate}[noitemsep]
	\item Estimate the predicted value of \textit{y} via the (general) linear model formula;
	\item Calculate the residuals;
	\item Calculate the SSE.
	\item The function should return the SSE.
\end{enumerate}

\lstinputlisting[language=R, firstline=66, lastline=71]{PS1MS.R} 

Using BFGS:
\lstinputlisting[language=R, firstline=73, lastline=77]{PS1MS.R} 

Exploring results:
\lstinputlisting[language=R, firstline=78, lastline=78]{PS1MS.R} 

\begin{verbatim}
	[1] 0.1391874 2.7266985
\end{verbatim}

	Comparing the results to the \texttt{lm} function: 
	
	\lstinputlisting[language=R, firstline=80, lastline=81]{PS1MS.R} 
	
	\begin{verbatim}
Call:
lm(formula = data$y ~ data$x)

Coefficients:
(Intercept)            data$x
       0.1392          2.7267 
	\end{verbatim}
	
\textbf{The results } are almost identical (again, "almost" due to rounding), and also close to the coefficients in the initial equation of \textit{y} = 0+2.75\textit{x}. \\

Also, as we have discussed during the lectures, the OLS method for a linear model is equivalent to the maximum likelihood estimation (MLE). Let's compare and prove this.
	\lstinputlisting[language=R, firstline=83, lastline=91]{PS1MS.R} 

Using BFGS:
	\lstinputlisting[language=R, firstline=93, lastline=98]{PS1MS.R}
	
Exploring results:
	\lstinputlisting[language=R, firstline=99, lastline=99]{PS1MS.R} 
	
	\begin{verbatim}
		[1] 0.1391874 2.7266985
	\end{verbatim}
	

\textbf{The results } are similar to what we got by using both the manually coded OLS regression function and  the \texttt{lm} function.

\end{document}
